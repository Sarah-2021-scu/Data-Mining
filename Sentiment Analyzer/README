In this project I have developed a pridictive Recurrent Neural Network model that can determine, given a review, whether it is positive or negative. The model used for sentiment analysis is Long short term memory, LSTM for its feedback connections and the results are analyzed using the F1 score.
Introduction: Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helps a business to understand the social sentiment of their brand, product or service while monitoring online conversations. A practical application in e-commerce applications is to infer sentiment (or polarity) from free-form review text submitted for a range of products. Sentiment Analysis is a procedure used to determine if a chunk of text is positive, negative or neutral. In text analytics, natural language processing (NLP) and machine learning (ML) techniques are combined to assign sentiment scores to the topics, categories or entities within a phrase.
RNN is one of the deep learning approaches which are used for sentiment analysis. It produces the output on the basis of previous computation by using sequential information. Recurrent neural networks (RNN) are a class of neural networks that are helpful in modeling sequence data. Derived from feedforward networks, RNNs exhibit similar behavior to how human brains function. i.e., recurrent neural networks produce predictive results in sequential data that other algorithms can't. Long short term memory or LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. 
Method
Model: The model implemented is an LSTM model using the python deep learning api keras. The model is dense in nature with 32 neurons in the hidden layer and 1 output layer. An Embedding layer is used to create word vectors for incoming words. It sits between the input and the LSTM layer, i.e. the output of the Embedding layer is the input to the LSTM layer. The 1D Global max pooling layer block is used to take a 2-dimensional tensor of size (input size) x (input channels) and computes the maximum of all the (input size) values for each of the (input channels).
Activation Functions: The activation function for the hidden layer is chosen as tanh and the output layer as sigmoid. Given the careful design of LSTM, ReLU was thought to not be appropriate for Recurrent Neural Networks (RNNs) such as the Long Short-Term Memory Network (LSTM) by default.
F1 Score: F1 Score, i.e. the Harmonic mean of precision and recall, is used to compute the accuracy of the test set prediction. It considers both Precision and Recall of the test to compute the score, and tell you how your system is performing.
Dataset: The dataset consists of the review text and asked to predict the sentiment for 25000 movie reviews provided in the test file. Positive sentiment is represented by a review rating of +1 and negative sentiment is represented by a review rating of -1. In test file, only reviews are provided and no ground truth rating is given. This data is used for comparing the output predictions.
Experiments
Preprocessing: The dataset is first converted into a dataframe of 2 columns; reviews and sentiment. The review text is further cleaned by removing the stop words and the words which do not contribute to the sentiment prediction. I have written a function text_cleaning for the preprocessing part. The 25000 reviews in the train.dat file are split into training and validation using the train_test_split function with test size as 0.3. For vectorizing the text corpus, a tokenizer of size 20000 is chosen and the dataset vocab is prepared. The validation text sequence is padded so as to make the length of the training and validation sequences the same. The labels are converted into a representation of 0 and 1 as the final layer is chosen to be sigmoid.
Model training: The model implemented using the specifications described in the previous section along with adam optimizer and binary cross entropy loss. The model is trained for 2 epochs. With validation loss = 0.289 and validation accuracy as 0.8865.
Testing: The test data reviews are similarly cleaned and preprocessed as the training and validation data. I have written a function for predicting the sentiments which includes padding of text sequences as well.
Results & Discussion: The model has been tested with a final 1 score of 0.882 and rank 6 on the leaderboard. Plots for validation and training accuracy and the validation and training loss are plotted in the notebook. The training loss is decreasing and the validation loss remains quite the same thus depicting goodness of the model. Similarly the training accuracy is increasing and the validation accuracy improves minimally. However, I have trained the model for Relu activation function as well and the accuracy turned out to be the same, though the LSTM model is not observed to be giving performance with Relu activation function. A further insight can be developed into why both the models were found to be performing the same.
