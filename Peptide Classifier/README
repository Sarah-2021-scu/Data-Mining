Introduction 
A Feed Forward Neural Network is an artificial neural network in which the connections between nodes do not form a cycle. It is the simplest form of neural network as information is only processed in one direction. While the data may pass through multiple hidden nodes, it always moves in one direction and never backwards. In this model, a series of inputs enter the layer and are multiplied by the weights. Each value is then added together to get a sum of the weighted input values. If the sum of the values is above a specific threshold, usually set at zero, the value produced is often 1, whereas if the sum falls below the threshold, the output value is -1. 
Method 
Model: The model implemented, as shown in figure 1, is a three-layer neural network with 38 neurons in the first layer, 15 in the second and 1 layer, i.e. the output neuron in the final layer. 
Activation Functions: The activation functions used are ReLU (Rectified Linear Unit) for the inner layer Sigmoid for the final layer. The choice of activation functions are based on two factors: 1) Sigmoid, present at the final layer of a network, because it helps push the output towards -1/0 or 1 (classifying the output into 2 classes, for example). When used in previous layers, it may suffer from vanishing gradient issues. 2) ReLU, used in the inner layers, keeps the model light (not dense) as many features may not contribute at all.
Loss Functions: Mean Square Error is often used in regression challenges, when the output of the network is a continuous value, for example: a temperature value or the cost of a house. However, this project deals with a different kind of challenge, a binary classification challenge, where our output will be either 0 or 1 (0 meaning benign, 1 meaning malignant). When working with classification challenges, there is a different loss function that works better at expressing the difference between our predicted output and our correct one. It is called the Cross-Entropy Loss Function which I have used in my network. 
Dataset: The dataset consists of a training dataset with 1566 records and the test dataset of 392 records. Training and test data contain peptide sequences, one per line in the file. Peptides are encoded as strings with characters from an alphabet of 20+ characters, each representing an amino-acid residue. The training set also includes the label for each sequence as 1 (antibiofilm) or -1 (not antibiofilm) as the first character in each line of the training file, separated from the sequence by a tab (\t) character.
Experiments 
Preprocessing: The dataset used is an imbalance dataset and thus to improve performance the negative class has been oversampled using the Random Sampler built in function in the Imblearn python package. The peptides or the strings of characters have been extracted from the dataset by creating a dataframe and the features have been extracted using the bag-of-words method. Since the total number of unique features are different for the training and testing set, a general vocabulary of 26 features have been created. The input is fed as feature vectors to the neural network with randomly chosen weights and biases.
Testing: The weights are optimized using the gradient descent optimization algorithm. Since the dataset is small, I have opted for gradient descent instead of batch gradient descent algorithm. The oversampled training dataset has been split into training and validation set in the ratio of 80:20, similar to training and test data. The first 570 samples have been chosen as the validation set while the rest are used for training. The selection of the first 570 samples as validation set has been due to the fact that both the minority and majority classes are fairly present in it. The learning rate is set to be 0.07 so as to ensure appropriate training and avoid overfitting. The cross-entropy loss is computed after every 500 iterations and the accuracy of predicted labels is determined for the training and the validation dataset. The model is further regularized using l2 regularization algorithm to balance the weights
Deep Learning Framework: The same model is implemented again using the open-source Python Library Keras. The choice of selection depends upon the consistency and simplicity of the API. The model is trained using the feature vectors from the oversampled data in batches of size 10 with 100% accuracy. Sequential model is used as it is the most appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor. Since each neuron in the layer receives input from all neurons of its previous layer, the Keras dense layer has been chosen for implementation.
Results & Discussion 
The confusion matrix for the training and validation set for different thresholds have been plotted and the accuracy has been reported in the code. The labels for test data are predicted using the artificial neural network framework with an accuracy of 95% and have been reported in the code file. The MMC score is found to be 0.8710. The model has been tested with various activation functions such as tanh but the performance was found best with sigmoid and ReLU. The results for the same are not a part of the submission but can be made available at request. It is also found that as the number of neurons increases the model accuracy is improved but the model converges too quickly. So, to ensure gradual learning and updating, the number of neurons are kept less than twice the maximum number of features.

